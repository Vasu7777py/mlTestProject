{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyTorch ver :  1.12.1+cu116\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"pyTorch ver : \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device acustic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 10 19:29:33 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 517.48       Driver Version: 517.48       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   61C    P0    27W /  N/A |    945MiB /  6144MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1540    C+G   ...zilla Firefox\\firefox.exe    N/A      |\n",
      "|    0   N/A  N/A      3544    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      7960    C+G   ...qxf38zg5c\\Skype\\Skype.exe    N/A      |\n",
      "|    0   N/A  N/A      8204    C+G   ...\\app-1.0.9006\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     15680    C+G   ..._dt26b99r8h8gj\\RtkUWP.exe    N/A      |\n",
      "|    0   N/A  N/A     15956    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18564    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     19760    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     20076    C+G   ...2gh52qy24etm\\Nahimic3.exe    N/A      |\n",
      "|    0   N/A  N/A     22824    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     22920    C+G   ...batNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     23596    C+G   ...pp-2.2236.10\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     25832    C+G   ...s\\Rainmeter\\Rainmeter.exe    N/A      |\n",
      "|    0   N/A  N/A     26488    C+G   ...zilla Firefox\\firefox.exe    N/A      |\n",
      "|    0   N/A  N/A     26756    C+G   ...qxf38zg5c\\Skype\\Skype.exe    N/A      |\n",
      "|    0   N/A  N/A     29184    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     29360    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     33900    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     33972    C+G   ...4.0.4.0\\GoogleDriveFS.exe    N/A      |\n",
      "|    0   N/A  N/A     34456    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     34516    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of images whether the image is of cat and dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Steps\n",
    "1. Create data set of 300+ images of cat and dog\n",
    "2. Load images as batch of 32\n",
    "3. Create a ml model to predict\n",
    "4. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean the data and create a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b1.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080p_cat_images.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2261526.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>235613.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>249470.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>_120248530_gettyimages-157037529.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>_120373298_gettyimages-1300362661.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>_124800860_gettyimages-1287712627.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>_124800861_gettyimages-469360172.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>_124800862_gettyimages-651322222.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1318 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   fileName class\n",
       "0                                   0b1.jpg   cat\n",
       "1                      1080p_cat_images.jpg   cat\n",
       "2                               2261526.jpg   cat\n",
       "3                                235613.jpg   cat\n",
       "4                                249470.jpg   cat\n",
       "...                                     ...   ...\n",
       "1313   _120248530_gettyimages-157037529.jpg   dog\n",
       "1314  _120373298_gettyimages-1300362661.jpg   dog\n",
       "1315  _124800860_gettyimages-1287712627.jpg   dog\n",
       "1316   _124800861_gettyimages-469360172.jpg   dog\n",
       "1317   _124800862_gettyimages-651322222.jpg   dog\n",
       "\n",
       "[1318 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = None\n",
    "dir = 'data'\n",
    "\n",
    "if (not os.path.isfile(os.path.join(dir, 'data.csv'))):\n",
    "\n",
    "  print(\"creating csv\")\n",
    "\n",
    "  subDir = os.listdir(dir)\n",
    "\n",
    "  dataDict = {}\n",
    "\n",
    "  for classDir in subDir:\n",
    "    if os.path.isdir(os.path.join(dir, classDir)):\n",
    "      dataDict[classDir] = os.listdir(os.path.join(dir, classDir))\n",
    "    else:\n",
    "      print(f\"not dir : {classDir}\")\n",
    "  \n",
    "  data = pd.DataFrame({\n",
    "    'fileName': [],\n",
    "    'class': []\n",
    "  })\n",
    "\n",
    "  for class_ in dataDict:\n",
    "    for fn in dataDict[class_]:\n",
    "      data.loc[len(data.index)] = [fn, class_]\n",
    "\n",
    "  csv = data.to_csv(os.path.join(dir, 'data.csv'), index=False)\n",
    "  \n",
    "else:\n",
    "\n",
    "  print(\"loading csv\")\n",
    "\n",
    "  data = pd.read_csv(os.path.join(dir, 'data.csv'))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, df, transform=None):\n",
    "    super(MyDataset, self).__init__()\n",
    "    self.df = df\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    path = os.path.join(dir, self.df.iloc[index, 1], self.df.iloc[index, 0])\n",
    "    \n",
    "    img = torchvision.io.read_image(path).to(device = device)\n",
    "\n",
    "    y_label = torch.tensor(0 if (self.df.iloc[index, 1] == 'cat') else 1).to(device = device)\n",
    "    \n",
    "    if self.transform:\n",
    "      img = self.transform(img)\n",
    "    \n",
    "    # # color_channel, width, height -> width, height, color_channel\n",
    "    # img = img.permute(1, 2, 0)\n",
    "    # divide by 255 so that max value will be 1, so that its easier for gpu to work with\n",
    "    img = img / torch.tensor(255).to(device = device)\n",
    "\n",
    "    return(img, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split the dataset into training, validation and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for the dataset\n",
    "transform = trans.Resize(size = (224, 224))\n",
    "\n",
    "# Create the dataset\n",
    "ds = MyDataset(data, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainSet count : 922,\n",
      " validationSet count : 263,\n",
      " testSet count : 133\n"
     ]
    }
   ],
   "source": [
    "trainCount = int(.7 * len(ds))\n",
    "validationCount = int(.2 * len(ds))\n",
    "testCount = len(ds) - trainCount - validationCount\n",
    "\n",
    "print(f'trainSet count : {trainCount},\\n validationSet count : {validationCount},\\n testSet count : {testCount}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainSet : <torch.utils.data.dataset.Subset object at 0x0000017D170F40D0>,\n",
      " validationSet : <torch.utils.data.dataset.Subset object at 0x0000017D170F4220>,\n",
      " testSet : <torch.utils.data.dataset.Subset object at 0x0000017D170F4130>\n"
     ]
    }
   ],
   "source": [
    "batchSize = 32\n",
    "numWorkers = 4\n",
    "\n",
    "trainDataSet, validationDataSet, testingDataSet = random_split(ds, (trainCount, validationCount, testCount))\n",
    "\n",
    "print(f'trainSet : {trainDataSet},\\n validationSet : {validationDataSet},\\n testSet : {testingDataSet}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x17d170f4970>,\n",
       " 'validation': <torch.utils.data.dataloader.DataLoader at 0x17d170f4f70>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x17d170f4e50>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataLoder = DataLoader(\n",
    "  trainDataSet,\n",
    "  batch_size=batchSize,\n",
    "  shuffle=True,\n",
    "  num_workers=0\n",
    ")\n",
    "\n",
    "validationDataLoder = DataLoader(\n",
    "  validationDataSet,\n",
    "  batch_size=batchSize,\n",
    "  shuffle=True,\n",
    "  num_workers=0\n",
    ")\n",
    "\n",
    "testDataLoder = DataLoader(\n",
    "  testingDataSet,\n",
    "  batch_size=batchSize,\n",
    "  shuffle=True,\n",
    "  num_workers=0\n",
    ")\n",
    "\n",
    "dataLoaders = {\n",
    "  'train': trainDataLoder,\n",
    "  'validation': validationDataLoder,\n",
    "  'test': testDataLoder\n",
    "}\n",
    "\n",
    "dataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModle (nn.Module):\n",
    "  def __init__ (self):\n",
    "    super(myModle, self).__init__()\n",
    "    #((w - f + 2p) / s) + 1 -> ((150 - 3 + 2*1) / 1) + 1 -> 224\n",
    "\n",
    "    #in shape = (32, 3, 224, 224)\n",
    "    self.l1 = nn.Conv2d(\n",
    "      in_channels=3, out_channels=12,\n",
    "      kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    #shape = (32, 12, 224, 224)\n",
    "    self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "    #shape = (32, 12, 224, 224)\n",
    "\n",
    "    self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "    # reduce the img by 2\n",
    "    # shape = (32, 12, 112, 112)\n",
    "\n",
    "    self.l2 = nn.Conv2d(\n",
    "      in_channels=12, out_channels=32,\n",
    "      kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    # shape = (32, 32, 112, 112)\n",
    "\n",
    "    self.l3 = nn.Conv2d(\n",
    "      in_channels=32, out_channels=64,\n",
    "      kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    # shape = (32, 64, 112, 112)\n",
    "    self.bn3 = nn.BatchNorm2d(num_features=64)\n",
    "\n",
    "    self.out = nn.Linear(in_features=64*112*112, out_features=2)\n",
    "\n",
    "    # cnn(3->12) -> bn -> pool(/2) -> cnn(12->32) -> cnn(32 -> 64) -> bn -> out([64*112*112]802816 -> 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    y = self.l1(x)\n",
    "    y = self.bn1(y)\n",
    "    y = nn.ReLU(y)\n",
    "\n",
    "    y = self.pool1(y)\n",
    "\n",
    "    y = self.l2(y)\n",
    "    y = nn.ReLU(y)\n",
    "\n",
    "    y = self.l3(y)\n",
    "    y = self.bn3(y)\n",
    "    y = nn.ReLU(y)\n",
    "\n",
    "    y = y.view(-1, 64*112*112)\n",
    "\n",
    "    y = self.out(y)\n",
    "\n",
    "    return y\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((224 - 3 + 2*1) / 1) + 1\n",
    "# 64*112*112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myModle(\n",
       "  (l1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (l2): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (l3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (out): Linear(in_features=802816, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = myModle().to(device= device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0.0001\n",
       " ),\n",
       " CrossEntropyLoss())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.0001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optim, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image file. Only jpeg and png are currently supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\myProjects\\mlTestProject\\imageClasf.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/myProjects/mlTestProject/imageClasf.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# training\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/myProjects/mlTestProject/imageClasf.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/myProjects/mlTestProject/imageClasf.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, (img, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataLoaders[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/myProjects/mlTestProject/imageClasf.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/myProjects/mlTestProject/imageClasf.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   y \u001b[39m=\u001b[39m model(img)\n",
      "File \u001b[1;32mc:\\Users\\Vasu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Vasu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Vasu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Vasu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Vasu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32md:\\myProjects\\mlTestProject\\imageClasf.ipynb Cell 24\u001b[0m in \u001b[0;36mMyDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/myProjects/mlTestProject/imageClasf.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/myProjects/mlTestProject/imageClasf.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39miloc[index, \u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39miloc[index, \u001b[39m0\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/myProjects/mlTestProject/imageClasf.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   img \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mread_image(path)\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/myProjects/mlTestProject/imageClasf.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m   y_label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39miloc[index, \u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/myProjects/mlTestProject/imageClasf.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n",
      "File \u001b[1;32mc:\\Users\\Vasu\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:252\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    250\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m    251\u001b[0m data \u001b[39m=\u001b[39m read_file(path)\n\u001b[1;32m--> 252\u001b[0m \u001b[39mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[1;32mc:\\Users\\Vasu\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:229\u001b[0m, in \u001b[0;36mdecode_image\u001b[1;34m(input, mode)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m    228\u001b[0m     _log_api_usage_once(decode_image)\n\u001b[1;32m--> 229\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mdecode_image(\u001b[39minput\u001b[39;49m, mode\u001b[39m.\u001b[39;49mvalue)\n\u001b[0;32m    230\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Vasu\\anaconda3\\lib\\site-packages\\torch\\_ops.py:143\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    139\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported image file. Only jpeg and png are currently supported."
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "los = []\n",
    "\n",
    "for e in range(epoch):\n",
    "\n",
    "  tacc = 0.0\n",
    "  tloss = 0.0\n",
    "  \n",
    "  # training\n",
    "  model.train()\n",
    "\n",
    "  for _, (img, label) in enumerate(dataLoaders['train']):\n",
    "    optim.zero_grad()\n",
    "    y = model(img)\n",
    "    l = loss(y, label)\n",
    "    l.backward()\n",
    "    optim.step()\n",
    "\n",
    "    tloss += l.cpu().data * img.size(0)\n",
    "    _, pred = torch.max(y.data, 1)\n",
    "    tacc += int(torch.sum(pred == label.data))\n",
    "\n",
    "  acc.append(tacc / len(trainDataSet))\n",
    "  los.append(tloss / len(trainDataSet))\n",
    "\n",
    "  if e % 100 == 0:\n",
    "    print(f'epoch : {e} | acc : {tacc / len(trainDataSet)} | loss : {tloss / len(trainDataSet)}')\n",
    "\n",
    "  # evaluation\n",
    "  model.eval()\n",
    "\n",
    "  tacc = 0.0\n",
    "  tloss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f1915f8c77532ce8e9ccb566c16c0a5834323517f435c95232d3dc03531b73e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
